

#' @title Sparse Integrative Discriminant Analysis for Multi-view Structured (Network) Data
#'
#' @description Performs sparse integrative disdcriminant analysis of multi-view structured
#'  (network) data to 1) obtain discriminant vectors that are associated and optimally
#'  separate subjects into different classes 2) estimate misclassification rate, and total
#'  correlation coefficient. The Laplacian of the underlying graph is used to smooth the
#'  discriminant vectors to encourage variables within a view that are connected to have a
#'  similar effect. Allows for the inclusion of other covariates which are not penalized in
#'  the algorithm. It is recommended to use cvSIDANet to choose best tuning parameter.
#'
#' @param Xdata A list with each entry containing training views of size \eqn{n \times p_d},
#' where \eqn{d = 1, \dots, D}. Rows are samples and columns are variables. If covariates
#' are available, they should be included as a separate view, and set as the last dataset.
#' For binary or categorical covariates (assumes no ordering), we suggest the use of
#' indicator variables.
#' @param Y \eqn{n \times 1} vector of class membership. Numeric, coded as 1, 2, ....
#' @param myedges A list with each entry containing a \eqn{M_d \times 2} matrix of edge information for
#' each view. If a view has no edge information, set to 0; this will default to SIDA. If
#' covariates are available as a view (Dth view), the edge information should be
#' set to 0.
#' @param myedgeweight A list with each entry containing a Md×1 vector of weight information for each
#' view. If a view has no weight information,set to 0; this will use the Laplacian
#' of an unweighted graph. If covariates are available as a view (Dth view), the
#' weight information should be set to 0.
#' @param Tau \eqn{d \times 1} vector of tuning parameter. It is recommended to use sidatunerange to
#' obtain lower and upper bounds for the tuning parameters since too large a tuning
#' parameter will result in a trivial solution vector (all zeros) and too small may
#' result in non-sparse vectors.
#' @param withCov TRUE or FALSE if covariates are available. If TRUE, please set all covariates as
#' one dataset and should be the last dataset. For binary and categorical variables,
#' use indicator matrices/vectors. Default is FALSE.
#' @param Xtestdata A list with each entry containing testing views of size \eqn{ntest \times p_d},
#' where \eqn{d =1, ..., D}. Rows are samples and columns are variables. The order of the list
#' should be the same as the order for the training data, Xdata. Use if you want to
#' predict on a testing dataset. If no Xtestdata, set to NULL.
#' @param Ytest \eqn{ntest \times 1} vector of test class membership. Numeric, coded as 1, 2, ....
#' If no testing data provided, set to NULL.
#' @param AssignClassMethod Classification method. Either Joint or Separate. Joint uses all
#' discriminant vectors from D datasets to predict class membership. Separate predicts class membership
#' separately for each dataset. Default is Joint.
#' @param plotIt TRUE or FALSE. If TRUE, produces discriminants and correlation plots. Default is FALSE.
#' @param standardize TRUE or FALSE. If TRUE, data will be normalized to have mean zero and
#' variance one for each variable. Default is TRUE.
#' @param maxiteration Maximum iteration for the algorithm if not converged.Default is 20.
#' @param weight Balances separation and association. Default is 0.5.
#' @param thresh Threshold for convergence. Default is 0.001.
#' @param eta Balances the selection of network, and variables within network. Default is 0.5.
#' @param mynormLaplacianG The normalized Laplacian of a graph. Set to NULL and this would be
#' estimated using edge matrix and edge weights.
#'
#' @details The function will return several R objects, which can be assigned to a variable.
#' To see the results, use the “$" operator.
#'
#' @return A list containing the following information:
#' \item{sidaneterror}{Estimated classification error. If testing data provided, this will be
#' test classification error, otherwise, training error}
#' \item{sidanetcorrelation}{Sum of pairwise RV coefficients. Normalized to be within 0 and 1, inclusive.}
#' \item{hatalpha}{A list of estimated sparse discriminant vectors for each view.}
#' \item{PredictedClass}{Predicted class. If AssignClassMethod=’Separate’, this will be a
#' \eqn{ntest \times D} matrix, with each column the predicted class for each data.}
#'
#' @seealso \code{\link{cvSIDANet}}
#' 
#' @importFrom CVXR norm diag
#'
#' @references
#' Sandra E. Safo, Eun Jeong Min, and Lillian Haine (2022), Sparse Linear Discriminant
#' Analysis for Multi-view Structured Data, Biometrics.
#'
#' @export
#' @examples
#' \dontrun{
#' ##---- read in data
#' data(sidanetData)
#'
#' ##---- call sidanet algorithm to estimate discriminant vectors, and predict on testing data
#' #call sidanettunerange to get range of tuning paramater
#' Xdata=sidanetData[[1]]
#' Y=sidanetData[[2]]
#' Xtestdata=sidanetData[[3]]
#' Ytest=sidanetData[[4]]
#' myedges=sidanetData[[5]]
#' myedgeweight=sidanetData[[6]]
#'
#' ngrid=10
#' mytunerange=sidanettunerange(Xdata,Y,ngrid,standardize=TRUE,weight=0.5,eta=0.5,
#'                              myedges,myedgeweight)
#'
#' # an example with Tau set as the lower bound
#' Tau=c(mytunerange$Tauvec[[1]][1], mytunerange$Tauvec[[2]][1])
#'
#' #example with two views having edge weights
#' mysidanet=sidanet(Xdata,Y,myedges,myedgeweight,Tau,Xtestdata=Xtestdata,Ytest=Ytest)
#'
#' test.error=mysidanet$sidaneterror
#' test.correlation=mysidanet$sidanetcorrelation
#' hatalpha=mysidanet$hatalpha
#' predictedClass=mysidanet$PredictedClass
#' }
sidanet=function(Xdata=Xdata,Y=Y,myedges=myedges,myedgeweight=myedgeweight,Tau=Tau,withCov=FALSE,Xtestdata=NULL,Ytest=NULL,AssignClassMethod='Joint',plotIt=FALSE,standardize=TRUE,maxiteration=20,weight=0.5,thresh= 1e-03,eta=0.5,mynormLaplacianG=NULL){


  #check inputs640
  dsizes=lapply(Xdata, function(x) dim(x))
  n=dsizes[[1]][1]
  nsizes=lapply(Xdata, function(x) dim(x)[1])

  if(all(nsizes!=nsizes[[1]])){
    stop('The datasets  have different number of observations')
  }

  #check data
  if (is.list(Xdata)) {
    D = length(Xdata)
  } else {
    stop("Input data should be a list")
  }

  if(is.null(Xtestdata)){
    Xtestdata=Xdata
  }

  #check inputs for testing data
  ntestsizes=lapply(Xtestdata, function(x) dim(x)[1])
  if(all(ntestsizes!=ntestsizes[[1]])){
    stop('The testing datasets  have different number of observations')
  }

  if(is.null(Ytest)){
    Ytest=Y
  }

  if(is.null(AssignClassMethod)){
    AssignClassMethod='Joint'
  }

  if(is.null(withCov)){
    withCov=FALSE
  }

  if(is.null(plotIt)){
    plotIt=FALSE
  }

  if(is.null(standardize)){
    standardize=TRUE
  }

  if(is.null(maxiteration)){
    maxiteration=20
  }

  if(is.null(weight)){
    weight=0.5
  }

  if(is.null(thresh)){
    thresh=1e-03
  }

  if(is.null(mynormLaplacianG)){
    mynormLaplacianG=myNLaplacianG(Xdata=Xdata,myedges=myedges,myedgeweight=myedgeweight)
  }

  if(is.null(eta)){
    eta=1e-03
  }

  #standardize if true
  if(standardize==TRUE){
    Xdata=lapply(Xdata,function(x)scale(x,center=TRUE,scale=TRUE))
    Xtestdata=lapply(Xtestdata,function(x)scale(x,center=TRUE,scale=TRUE))
  }


  nK=length(unique(as.vector(Y))) -1


  #norm function for convergence
  normdiff=function(xnew,xold){
    ndiff=CVXR::norm(xnew-xold,'f')^2 / CVXR::norm (xold,'f')^2
  }
  #initialize
  iter=0
  diffalpha=1
  reldiff=1
  relObj=1

  mynsparse=myfastIDAnonsparse(Xdata,Y,weight)
  myalpha=mynsparse$myalphaoldmat
  #while convergence is not met
  while(iter < maxiteration && min(min(reldiff,relObj),max(diffalpha))> thresh){
    iter=iter+1
    myalphaold=myalpha
    mysidainner=sidanetinner(Xdata,Y,mynsparse$sqrtminvmat,
                             myalphaold,mynsparse$tildealphamat, 
                             mynsparse$tildelambda,Tau,weight,eta,
                             myedges,myedgeweight,mynormLaplacianG,withCov)

    myalpha=mysidainner$hatalpha
    nz=sapply(1:D, function(i) list(colSums(myalpha[[i]]!=0)))
    nz=cbind(c(do.call(rbind,nz)))
    if(any(nz==0)){
      myalpha=myalphaold
      break
    }
    diffalpha=mapply(normdiff, myalpha, myalphaold)
    sumnormdiff=sum(sapply(1:D, function(d) CVXR::norm(myalpha[[d]]-myalphaold[[d]],'f')^2 ))
    sumnormold=sum(sapply(1:D, function(d) CVXR::norm(myalphaold[[d]],'f')^2    ))
    reldiff=sumnormdiff/sumnormold
    ObjNew=sum(sapply(1:D, function(d) (1-eta)*CVXR::norm(myalpha[[d]],'f')^2 + eta*CVXR::norm(as.matrix(mysidainner$myL[[d]])%*%myalpha[[d]],'f')^2))
    ObjOld=sum(sapply(1:D, function(d) (1-eta)*CVXR::norm(myalphaold[[d]],'f')^2 + eta*CVXR::norm(as.matrix(mysidainner$myL[[d]])%*%myalphaold[[d]],'f')^2))
    relObj=abs(ObjNew-ObjOld)/ObjOld
  }

  #classification
  if(AssignClassMethod=='Joint'){
    myclassify=sidaclassify(myalpha,Xtestdata,Xdata,Y,AssignClassMethod='Joint')
    sidaneterror=sum(myclassify$PredictedClass!=Ytest)/length(Ytest)
  }else if(AssignClassMethod=='Separate'){
    sidaneterror=sapply(1:(nK+1), function(x)  sum(myclassify$PredictedClass[,x]!=Ytest)/length(Ytest) )
  }


  #sum pairwise correlations of training data
  ss=list()
  #sum pairwise correlations
  for(d in 1:D){
    dd=setdiff(seq(1, D, by= 1),d)
    #correlations
    sumCorr2=0;
    for (jj in 1:length(dd)){
      j=dd[jj];
      X1=Xtestdata[[d]]%*%myalpha[[d]]
      X2=Xtestdata[[j]]%*%myalpha[[j]]
      X1=scale(X1, center=TRUE,scale=FALSE)
      X2=scale(X2, center=TRUE,scale=FALSE)
      X1X2=t(X1)%*%X2/dim(X1)[1]
      X1X1=t(X1)%*%X1/dim(X1)[1]
      X2X2=t(X2)%*%X2/dim(X2)[1]
      sumcorr3=sum(CVXR::diag(X1X2%*%t(X1X2)))/(sqrt(sum(CVXR::diag(X1X1%*%X1X1)))*sqrt(sum(CVXR::diag(X2X2%*%X2X2))))
      sumCorr2=sumCorr2+sumcorr3
    }
    ss[[d]]=sumCorr2/length(dd)
  }

  sidanetcorrelation=sum(do.call(rbind,ss))/D


  result=list(sidaneterror=sidaneterror,sidanetcorrelation=sidanetcorrelation,hatalpha=myalpha,
              PredictedClass=myclassify$PredictedClass)

  class(result)="SIDANet"
  return(result)

}


#' @title Cross validation for Sparse Integrative Discriminant Analysis
#' for Multi-view Structured (Network) Data
#'
#' @description Peforms nfolds cross validation to select optimal tuning
#' parameters for sidanet based on training data, which are then used with
#' the training or testing data to predict class membership. Allows for
#' inclusion of covariates which are not penalized. If you want to apply
#' optimal tuning parameters to testing data, you may also use sidanet.
#'
#' @param Xdata A list with each entry containing training views of size
#' \eqn{n \times p_d}, where \eqn{d = 1, \dots, D}. Rows are samples and
#' columns are variables. If covariates are avail- able, they should be
#'  included as a separate view, and set as the last dataset. For binary
#'  or categorical covariates (assumes no ordering), we suggest the use of
#'  indicator variables.
#' @param Y \eqn{n \times 1} vector of class membership.
#' @param myedges  A list with each entry containing a \eqn{M_d \times 2}
#' matrix of edge information for each view. If a view has no edge information,
#' set to 0; this will default to SIDA. If covariates are available as a view
#' (\eqn{D}th view), the edge information should be set to 0.
#' @param myedgeweight A list with each entry containing a \eqn{M_d \times 1}
#' vector of weight information for each view. If a view has no weight information,
#' set to 0; this will use the Laplacian of an unweighted graph. If covariates
#' are available as a view (\eqn{D}th view), the weight information should be set to 0.
#' @param withCov TRUE or FALSE if covariates are available. If TRUE, please set all
#'  covariates as one dataset and should be the last dataset. For binary and categorical
#'  variables, use indicator matrices/vectors. Default is FALSE.
#' @param plotIt TRUE or FALSE. If TRUE, produces discriminants and correlation plots.
#' De- fault is FALSE.
#' @param Xtestdata A list with each entry containing testing views of size
#' \eqn{ntest \times p_d}, where \eqn{d = 1, \dots, D}. Rows are samples and columns are
#'  variables. The order of the list should be the same as the order for the training data,
#'  Xdata. Use if you want to predict on a testing dataset. If no Xtestdata, set to NULL.
#' @param Ytest \eqn{ntest \times 1} vector of test class membership. Numeric, coded as 1, 2, ....
#' If no testing data provided, set to NULL.
#' @param isParallel TRUE or FALSE for parallel computing. Default is TRUE.
#' @param ncores Number of cores to be used for parallel computing. Only used if
#' isParallel=TRUE. If isParallel=TRUE and ncores=NULL, defaults to half the size of the
#'number of system cores.
#' @param gridMethod GridSearch or RandomSearch. Optimize tuning parameters over full grid or
#' random grid. Default is RandomSearch.
#' @param AssignClassMethod Classification method. Either Joint or Separate. Joint uses all
#' discriminant vectors from \eqn{D} datasets to predict class membership. Separate predicts
#' class membership separately for each dataset. Default is Joint
#' @param nfolds Number of cross validation folds. Default is 5.
#' @param ngrid Number of grid points for tuning parameters. Default is 8 for each view if
#'  \eqn{D =2}. If \eqn{D > 2}, default is 5.
#' @param standardize TRUE or FALSE. If TRUE, data will be normalized to have mean zero and
#' variance one for each variable. Default is TRUE.
#' @param maxiteration Maximum iteration for the algorithm if not converged. Default is 20.
#' @param weight Balances separation and association. Default is 0.5.
#' @param thresh Threshold for convergence. Default is 0.001.
#' @param eta Balances the selection of network, and variables within network. Default is 0.5.
#'
#' @details The function will return several R objects, which can be assigned to a variable.
#' To see the results, use the “$" operator.
#'
#' @return
#' A list containing the following information:
#' \item{sidaerror}{Estimated classication error. If testing data provided, this will be test
#' classification error, otherwise, training error}
#' \item{sidacorrelation}{Sum of pairwise RV coefficients. Normalized to be within 0
#' and 1, inclusive.}
#' \item{hatalpha}{A list of estimated sparse discriminant vectors for each view.}
#' \item{PredictedClass}{Predicted class for test data. If AssignClassMethod=’Separate’, this will
#' be a \eqn{ntest \times D} matrix, with each column the predicted class for each data.}
#' \item{PredictedClass.train}{Predicted class for train data. If AssignClassMethod=’Separate’, this will
#' be a \eqn{ntrain \times D} matrix, with each column the predicted class for each data.}
#' \item{optTau}{Optimal tuning parameters for each view, not including covariates,
#' if available.}
#' \item{gridValues}{Grid values used for searching optimal tuning paramters.}
#' \item{AssignClassMethod}{Classification method used. Joint or Separate.}
#' \item{gridMethod}{Grid method used. Either GridSearch or RandomSearch}
#'
#' @seealso \code{\link{sidanet}}
#' @importFrom CVXR norm diag
#' @importFrom parallel makeCluster stopCluster
#' @importFrom parallelly availableCores
#' @importFrom doParallel registerDoParallel
#' @importFrom foreach foreach
#'
#' @references
#' Sandra E. Safo, Eun Jeong Min, and Lillian Haine (2022), Sparse Linear Discriminant
#' Analysis for Multi-view Structured Data, Biometrics.
#'
#' @export
#' @examples
#' \dontrun{
#' ##---- read in data
#' data(sidanetData)
#'
#' ##---- call sidanet algorithm to estimate discriminant vectors, and predict on testing data
#' #call sidanettunerange to get range of tuning paramater
#' Xdata=sidanetData[[1]]
#' Y=sidanetData[[2]]
#' Xtestdata=sidanetData[[3]]
#' Ytest=sidanetData[[4]]
#' myedges=sidanetData[[5]]
#' myedgeweight=sidanetData[[6]]
#'
#'  mycv=cvSIDANet(Xdata,Y,myedges,myedgeweight,withCov=FALSE,plotIt=FALSE,Xtestdata=Xtestdata,
#'     Ytest=Ytest,isParallel=FALSE,ncores=NULL,gridMethod='RandomSearch',
#'     AssignClassMethod='Joint',nfolds=5,ngrid=8,standardize=TRUE,
#'     maxiteration=20, weight=0.5,thresh=1e-03,eta=0.5)
#'
#' #check output
#' test.error=mycv$sidaneterror
#' test.correlation=mycv$sidanetcorrelation
#' optTau=mycv$optTau
#' hatalpha=mycv$hatalpha
#' }
cvSIDANet=function(Xdata=Xdata,Y=Y,myedges=myedges,myedgeweight=myedgeweight,withCov=FALSE,plotIt=FALSE,Xtestdata=NULL,Ytest=NULL,isParallel=TRUE,ncores=NULL,gridMethod='RandomSearch',AssignClassMethod='Joint',nfolds=5,ngrid=8,standardize=TRUE,maxiteration=20, weight=0.5,thresh=1e-03,eta=0.5){

  starttimeall=Sys.time()
  XdataOrig=Xdata
  XtestdataOrig=Xtestdata
  YOrig=Y
  YtestOrig=Ytest

  #check data
  if (is.list(Xdata)) {
    D = length(Xdata)
    if(D==1){
      stop("There should be at least two datasets")
    }
  } else {
    stop("Input data should be a list")
  }


  #set defaults
  #If testing data are not provided, the default is to use training data
  if(is.null(Xtestdata)){
    Xtestdata=Xdata
  }

  if(is.null(Ytest)){
    Ytest=Y
  }

  #check inputs for training data
  dsizes=lapply(Xdata, function(x) dim(x))
  n=dsizes[[1]][1]
  nsizes=lapply(Xdata, function(x) dim(x)[1])

  if(all(nsizes!=nsizes[[1]])){
    stop('The  datasets  have different number of observations')
  }

  #check inputs for testing data
  ntestsizes=lapply(Xtestdata, function(x) dim(x)[1])
  if(all(ntestsizes!=ntestsizes[[1]])){
    stop('The testing datasets  have different number of observations')
  }


  #check data
  if (is.list(Xdata)) {
    D = length(Xdata)
    if(D==1){
      stop("There should be at least two datasets")
    }
  } else {
    stop("Input data should be a list")
  }

  if(is.null(withCov)){
    withCov=FALSE
  }

  if(is.null(plotIt)){
    plotIt=FALSE
  }

  if(is.null(standardize)){
    standardize=TRUE
  }

  #standardize if true
  if(standardize==TRUE){
    Xdata=lapply(Xdata,function(x)scale(x,center=TRUE,scale=TRUE))
    Xtestdata=lapply(Xtestdata,function(x)scale(x,center=TRUE,scale=TRUE))
  }

  if(is.null(gridMethod)){
    gridMethod='RandomSearch'
  }

  if(is.null(AssignClassMethod)){
    AssignClassMethod='Joint'
  }

  if(is.null(isParallel)){
    isParallel=TRUE
  }

  if(is.null(nfolds)){
    nfolds=5
  }

  if(is.null(ngrid)){
    ngrid=8
  }

  if(is.null(maxiteration)){
    maxiteration=20
  }

  if(is.null(weight)){
    weight=0.5
  }

  if(is.null(thresh)){
    thresh=1e-03
  }

  if(is.null(eta)){
    eta=1e-03
  }


  set.seed(1234)
  nK=length(unique(as.vector(Y))) -1

  nc=length(unique(as.vector(Y)))
  Nn=mat.or.vec(nc,1)
  foldid=list()
  for(i in 1:nc)
  {
    Nn[i]=sum(Y==i)
    mod1=Nn[i]%%nfolds
    if(mod1==0){
      foldid[[i]]=sample(c(rep(1:nfolds,times=floor(Nn[i])/nfolds)),Nn[i])
    }else if(mod1> 0){
      foldid[[i]]=sample(c(rep(1:nfolds,times=floor(Nn[i])/nfolds), 1:(Nn[i]%%nfolds)),Nn[i])
    }
  }

  foldid=unlist(foldid)

  #obtain tuning range common to all K


  if(withCov==TRUE){
    #Dnew=D-1
    Dnew=D
  }else if(withCov==FALSE){
    Dnew=D
  }

  if(Dnew>2){
    ngrid=5
  }

  starttimetune=Sys.time()
  print('Getting tuning grid values')
  #calculate the normalized laplacian
  mynormLaplacianG=myNLaplacianG(Xdata=Xdata,myedges=myedges,myedgeweight=myedgeweight)
  myTauvec=sidanettunerange(Xdata,Y,ngrid,standardize,weight,eta,myedges,myedgeweight,withCov)
  endtimetune=Sys.time()
  print('Completed at time')
  print(endtimetune-starttimetune)

  #define the grid
  mygrid=expand.grid(do.call(cbind,myTauvec))
  gridcomb=dim(mygrid)[1]
  if(gridMethod=='RandomSearch'){
    if(Dnew<2){
      ntrials=ngrid}
    else if(Dnew==2){
      ntrials=floor(0.2*gridcomb)}
    else if(Dnew>2){
      ntrials=floor(0.15*gridcomb)
    }
    mytune=sample(1:gridcomb, ntrials, replace = FALSE)
    gridValues=mygrid[mytune,]
  }else if(gridMethod=='GridSearch'){
    gridValues=mygrid
  }


  #  start_time=Sys.time()
  starttimeCV=Sys.time()
  counter=0
  gridValues=as.matrix(gridValues)
  CVOut=matrix(0, nfolds, nrow(as.matrix(gridValues)))
  #cross validation
  if(isParallel==TRUE){
    cat("Begin", nfolds,"-folds cross-validation", "\n")
    doParallel::registerDoParallel()
    if(is.null(ncores)){
      ncores=parallelly::availableCores()
      ncores=ceiling(ncores/2)}
    cl=parallel::makeCluster(ncores)
    #start_time=Sys.time()
    doParallel::registerDoParallel(cl)
    CVOut=matrix(0, nrow(gridValues), nfolds)
    mycv=foreach::foreach(i = 1:nrow(gridValues), .combine='rbind',.export=c('sidanet','sidanetinner','myfastinner','myfastIDAnonsparse','mysqrtminv','sidaclassify', 'sidanettunerange','DiscriminantPlots','CorrelationPlots'),.packages=c('CVXR','RSpectra','igraph','Matrix')) %dopar% {
      mTau=sapply(1:D, function(itau) list(t(gridValues[,itau][i])))
      #start_time=Sys.time()
      CVOut[i,]= sapply(1:nfolds, function(j){
        testInd=which(foldid==j)
        testX=lapply(Xdata, function(x) x[testInd,])
        testY=Y[testInd]
        trainX=lapply(Xdata, function(x) x[-testInd,])
        trainY=Y[-testInd]
        # counter=counter +1
        # #cat("Begin CV-fold", j, "\n")
        # print(paste("Begin CV-fold",1))
        mysida=sidanet(trainX,trainY,myedges,myedgeweight,mTau,withCov,Xtestdata=testX,Ytest=testY,AssignClassMethod,plotIt=NULL,standardize,maxiteration,weight,thresh,eta,mynormLaplacianG)
        return(min(mysida$sidaneterror))
      } )
    }
    #end_time=Sys.time()
    #SIDAtime=end_time-start_time
    CVOut=t(mycv)
    parallel::stopCluster(cl)
  }else if(isParallel==FALSE){
    CVOut=matrix(0, nfolds, nrow(gridValues))
    for (i in 1:nfolds){
      testInd=which(foldid==i)
      testX=lapply(Xdata, function(x) x[testInd,])
      testY=Y[testInd]
      trainX=lapply(Xdata, function(x) x[-testInd,])
      trainY=Y[-testInd]

      cat("Begin CV-fold", i, "\n")
      CVOut[i,]= sapply(1:nrow(gridValues), function(itau){
        mTau=sapply(1:D, function(d) list(t(gridValues[itau,][d])))
        mysida=sidanet(trainX,trainY,myedges,myedgeweight,mTau,withCov,Xtestdata=testX,Ytest=testY,AssignClassMethod,plotIt=NULL,standardize,maxiteration,weight,thresh,eta,mynormLaplacianG)
        return(min(mysida$sidaneterror))
      } )
    }
  }
  endtimeCV=Sys.time()
  print('Cross-validation completed at time')
  print(endtimeCV-starttimeCV)


  print('Getting Results......')
  #compute average classification error
  minEorrInd=max(which(colMeans(CVOut)==min(colMeans(CVOut))))
  optTau=gridValues[ minEorrInd,]

  #Apply on testing data
  moptTau=sapply(1:D, function(i) list(t(gridValues[minEorrInd,][i])))
  mysida=sidanet(Xdata=Xdata,Y=Y,myedges,myedgeweight,Tau=moptTau,withCov,Xtestdata=Xtestdata,Ytest=Ytest,AssignClassMethod,plotIt=NULL,standardize,maxiteration,weight,thresh,eta,mynormLaplacianG)

  #Apply on training data
  mysidaTrain=sidanet(Xdata=Xdata,Y=Y,myedges,myedgeweight,Tau=moptTau,withCov,Xtestdata=Xdata,Ytest=Y,AssignClassMethod,plotIt=NULL,standardize,maxiteration,weight,thresh,eta,mynormLaplacianG)


  ss=list()
  #sum pairwise RV coefficients
  for(d in 1:D){
    dd=setdiff(seq(1, D, by= 1),d)
    #correlations
    sumCorr2=0;
    for (jj in 1:length(dd)){
      j=dd[jj];
      X1=Xtestdata[[d]]%*%mysida$hatalpha[[d]]
      X2=Xtestdata[[j]]%*%mysida$hatalpha[[j]]
      X1=scale(X1, center=TRUE,scale=FALSE)
      X2=scale(X2, center=TRUE,scale=FALSE)
      X1X2=t(X1)%*%X2/dim(X1)[1]
      X1X1=t(X1)%*%X1/dim(X1)[1]
      X2X2=t(X2)%*%X2/dim(X2)[1]
      sumcorr3=sum(CVXR::diag(X1X2%*%t(X1X2)))/(sqrt(sum(CVXR::diag(X1X1%*%X1X1)))*sqrt(sum(CVXR::diag(X2X2%*%X2X2))))
      sumCorr2=sumCorr2+sumcorr3
    }
    ss[[d]]=sumCorr2/length(dd)
  }

  sidanetcorrelation=sum(do.call(rbind,ss))/D

  ss=list()
  #sum pairwise RV coefficients for training data
  for(d in 1:D){
    dd=setdiff(seq(1, D, by= 1),d)
    #correlations
    sumCorr2=0;
    for (jj in 1:length(dd)){
      j=dd[jj];
      X1=Xdata[[d]]%*%mysida$hatalpha[[d]]
      X2=Xdata[[j]]%*%mysida$hatalpha[[j]]
      X1=scale(X1, center=TRUE,scale=FALSE)
      X2=scale(X2, center=TRUE,scale=FALSE)
      X1X2=t(X1)%*%X2/dim(X1)[1]
      X1X1=t(X1)%*%X1/dim(X1)[1]
      X2X2=t(X2)%*%X2/dim(X2)[1]
      sumcorr3=(sum(CVXR::diag(X1X2%*%t(X1X2)))/
                  (sqrt(sum(CVXR::diag(X1X1%*%X1X1)))*
                     sqrt(sum(CVXR::diag(X2X2%*%X2X2)))))
      sumCorr2=sumCorr2+sumcorr3
    }
    ss[[d]]=sumCorr2/length(dd)
  }

  sidanetcorrelation.train=sum(do.call(rbind,ss))/D

  #Produce discriminant and correlation plot if plotIt=T
  if(plotIt==TRUE){
    DiscriminantPlots(Xtestdata,Ytest,mysida$hatalpha)
    CorrelationPlots(Xtestdata,Ytest,mysida$hatalpha)
  }else{
    myDiscPlot=NULL
    myCorrPlot=NULL

  }

  #print out some results
  cat("Estimated Test Classification Error is", mysida$sidaneterror, "\n")
  cat("Estimated Train Classification Error is", mysidaTrain$sidaneterror, "\n")

  cat("Estimated Test Correlation is", sidanetcorrelation, "\n")
  cat("Estimated Train Correlation is", sidanetcorrelation.train, "\n")

  #number of selected variables
  mysum=matrix(NA,nrow=D,ncol=1)
  hatalpha.temp=list()
  hatalpha=mysida$hatalpha
  for(d in 1:D){
    if(dim(hatalpha[[d]])[2] == 1){
      mysum[d,1]=sum(hatalpha[[d]]!=0)
    }else if (dim(hatalpha[[d]])[2] >1){
      hatalpha.temp[[d]]=rowSums(abs(hatalpha[[d]]))
      mysum[d,1]=sum(hatalpha.temp[[d]]!=0)
    }
    #cat("Number of nonzero coefficients in view", d, "is", sum(mysida$hatalpha[[d]]!=0), "\n")
    cat("Number of nonzero coefficients in view", d, "is", mysum[d,1], "\n")
  }

  for(d in 1:D){
    cat("Number of nonzero coefficients in view", d, "is", sum(mysida$hatalpha[[d]]!=0), "\n")
  }


  endtimeall=Sys.time()
  print("Total time used is")
  print(endtimeall-starttimeall)


  result=list(CVOut=CVOut,sidaneterror=mysida$sidaneterror,sidanetcorrelation=sidanetcorrelation,sidaneterror.train=mysidaTrain$sidaneterror,sidanetcorrelation.train=sidanetcorrelation.train,
              hatalpha=mysida$hatalpha,PredictedClass=mysida$PredictedClass,
              PredictedClass.train=mysidaTrain$PredictedClass,
              optTau=moptTau,gridValues=gridValues, AssignClassMethod=AssignClassMethod,
              gridMethod=gridMethod,
              InputData=XdataOrig)
  class(result)="SIDANet"
  return(result)
}


#' @title Tuning paramter grid values for sidanet
#'
#' @description Sidanet function to provide tuning parameter grid values for each
#' view, not including covariates, if available. It is recommended to use this to
#' get lower and upper bounds of tuning parameters for each view that can be used
#' in sidanet. This function is called by cvSIDANet to select optimal tuning parameters.
#'
#' @param Xdata A list with each entry containing training views of size \eqn{n \times p_d},
#' where \eqn{d = 1, \dots, D}. Rows are samples and columns are variables. If covariates
#' are available, they should be included as a separate view, and set as the last dataset.
#' For binary or categorical covariates (assumes no ordering), we suggest the use of
#' indicator variables.
#' @param Y \eqn{n \times 1} vector of class membership. Numeric, coded as 1, 2, ....
#' @param ngrid Number of grid points for tuning parameters.
#' @param standardize TRUE or FALSE. If TRUE, data will be normalized to have mean
#' zero and variance one for each variable. Default is TRUE.
#' @param weight Balances separation and association. Default is 0.5.
#' @param eta Balances the selection of network, and variables within network.
#' Default is 0.5.
#' @param myedges  A list with each entry containing a \eqn{M_d \times 2}
#' matrix of edge information for each view. If a view has no edge information,
#' set to 0; this will default to SIDA. If covariates are available as a view
#' (\eqn{D}th view), the edge information should be set to 0.
#' @param myedgeweight A list with each entry containing a \eqn{M_d \times 1}
#' vector of weight information for each view. If a view has no weight information,
#' set to 0; this will use the Laplacian of an unweighted graph. If covariates
#' are available as a view (\eqn{D}th view), the weight information should be set to 0.
#' @param withCov TRUE or FALSE if covariates are available. If TRUE, please set all
#'  covariates as one dataset and should be the last dataset. For binary and categorical
#'  variables, use indicator matrices/vectors. Default is FALSE.
#'
#' @details The function will return several R objects, which can be assigned to a variable.
#' To see the results, use the “$" operator.
#'
#' @importFrom CVXR norm diag
#' @importFrom stats quantile
#'
#' @return
#' \item{Tauvec}{Grid values for each data, not including covariates, if available.}
#'
#' @seealso \code{\link{sidanet}}
#'
#' @references
#' Sandra E. Safo, Eun Jeong Min, and Lillian Haine (2022), Sparse Linear Discriminant
#' Analysis for Multi-view Structured Data, Biometrics.
#'
#' @export
#' @examples
#' \dontrun{
#' ##---- read in data
#' data(sidanetData)
#'
#' ##---- call sidanet algorithm to estimate discriminant vectors, and predict on testing data
#' #call sidanettunerange to get range of tuning paramater
#' Xdata=sidanetData[[1]]
#' Y=sidanetData[[2]]
#' Xtestdata=sidanetData[[3]]
#' Ytest=sidanetData[[4]]
#' myedges=sidanetData[[5]]
#' myedgeweight=sidanetData[[6]]
#'
#' ngrid=10
#' mytunerange=sidanettunerange(Xdata,Y,ngrid,standardize=TRUE,weight=0.5,eta=0.5,
#'                              myedges,myedgeweight)
#'
#' # an example with Tau set as the lower bound
#' Tau=c(mytunerange$Tauvec[[1]][1], mytunerange$Tauvec[[2]][1])
#'
#' #example with two views having edge weights
#' mysidanet=sidanet(Xdata,Y,myedges,myedgeweight,Tau,Xtestdata=Xtestdata,Ytest=Ytest)
#'
#' test.error=mysidanet$sidaneterror
#' test.correlation=mysidanet$sidanetcorrelation
#' hatalpha=mysidanet$hatalpha
#' predictedClass=mysidanet$PredictedClass
#' }
sidanettunerange=function(Xdata=Xdata,Y=Y,ngrid=8,standardize=TRUE,weight=0.5,eta=0.5,myedges=myedges,myedgeweight=myedgeweight,withCov=FALSE){


  #check size of each data

  dsizes=lapply(Xdata, function(x) dim(x))
  n=dsizes[[1]][1]
  p=lapply(Xdata, function(x) dim(x)[2])
  D=length(dsizes)

  if(is.null(withCov)){
    withCov=FALSE
  }

  if(is.null(ngrid)){
    ngrid=8
  }

  if(is.null(standardize)){
    standardize=TRUE
  }

  if(is.null(weight)){
    weight=0.5
  }

  if(is.null(eta)){
    eta=1e-03
  }

  # if(withCov==TRUE){
  #   D=D-1
  # }

  nK=length(unique(as.vector(Y))) -1

  #standardize if true
  if(standardize==TRUE){
    Xdata=lapply(Xdata,function(x)scale(x,center=TRUE,scale=TRUE))
  }

  #create normalized Laplacian of Graph
  mynormLaplacianG=myNLaplacianG(Xdata=Xdata,myedges=myedges,myedgeweight=myedgeweight)
  #obtain nonsparse solutions
  mynsparse=myfastIDAnonsparse(Xdata,Y,weight)
  myfinner=myfastinner(Xdata,Y,mynsparse$sqrtminvmat,mynsparse$myalphaoldmat,mynsparse$tildealphamat, weight)


  #obtain upper and lower bounds
  ubx=lapply(myfinner$SepAndAssocd, function(x) CVXR::norm(x,'i')/1.2)
  lbx=lapply(1:D, function(x) 1.2*sqrt(log(p[[x]])/n)*ubx[[x]])
  ubx=lapply(1:D, function(x) ubx[[x]])

  #tuning range for each data
  Taugrid=list()
  cc=lapply(1, function(x1,x2)  cbind(lbx,ubx))

  cc=as.matrix(do.call(rbind,cc))
  for(d in 1:D){
    Taugrid[[d]]=seq(as.numeric(cc[d,1]),as.numeric(cc[d,2]),length.out=(ngrid+1))
  }

  #about 25% sparsity

  myperx=lapply(Taugrid, function(x) stats::quantile(x[1:ngrid], c(.1, .15, .2, .25, .35, .45), type=5))#similar to matlab
  myperx2=do.call(rbind,myperx)
  for(loc in 1:6){
    mTaux=sapply(1:D, function(i) list(t(myperx2[i,loc])))
    myres=sidanetinner(Xdata,Y,mynsparse$sqrtminvmat,mynsparse$myalphaoldmat,mynsparse$tildealphamat, mynsparse$tildelambda,mTaux,weight,eta,myedges,myedgeweight,mynormLaplacianG,withCov)
    nnz=sapply(1:D, function(i) list(colSums(myres$hatalpha[[i]]!=0)/dsizes[[i]][2]))
    nnz=cbind(c(do.call(rbind,nnz))) #turns into a vector
    if(all(nnz<=0.25)){
      break
    }
  }
  #final grid
  Tauvec=sapply(1:D, function(i) list(seq(as.numeric(t(myperx2[i,loc])),as.numeric(ubx[[i]]),len=(ngrid+1))))
  Tauvec=sapply(1:D, function(x) list(Tauvec[[x]][1:ngrid]))

  result=list(Tauvec=Tauvec)
  return(result)
}

